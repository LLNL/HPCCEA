## RabbitMQ-Kafka Use Case
### Developed by Prabhleen Bagri, Lindsey Amaro, and David Fox
During our HPC Cluster Academy internship investigating RabbitMQ and Kafka, we wanted to develop a use case involving both of these message brokers. Since these services can be used to send tasks, in addition to messages, we decided to explore using RabbitMQ and Kafka to handle jobs issued by syslog.

This process begins with a Kafka producer reading from syslog messages. It parses all syslog messages and loops through looking for messages that request a certain job be done by a user or scheduling service. The producer publishes that message to a Kafka topic, from where a Kafka consumer reads the message and passes it to a RabbitMQ producer. That producer then delegates the job to the appropriate node (for example, delegating a GPU job to a RabbitMQ agent on a node suited for GPU work). Once the node completes the job, the RabbitMQ agent sends a task completion message to syslog. The Kafka producer, still reading from syslog, publishes that completion message to the same Kafka topic the job request was published on. Simultaneously, a Kafka reporter program is running. Once started, it reads from that topic and provides a sequential record of when tasks were issued and completed. Overall, this use case represents a method of handling jobs in a cluster. 

The files included here are an exploration of this use case, and a deployment of the process would require further refinement. All task messages are of the format "TASK: [GPU -or- Large Memory] Job Requested". All tasks being sent are simulated tasks, though this code could be expanded upon to send real jobs as well. Within this folder, task-syslog-producer.py acts as the Kaka producer; consumerDraft.py the Kafka consumer and the RabbitMQ producer; do_task.py the RabbitMQ agent; and reporter.py the Kafka reporter program. Each of these files requires additional command line arguments; on your terminal, type "python3 [file_name] -h" for argument options.
